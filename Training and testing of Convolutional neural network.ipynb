{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all the required libraries are added and then saved X and y are loaded with the help of pickle. Then, the X and y are converted into numpy arrey to feed into Convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "X = pickle.load(open(\"X.pickle\",\"rb\"))    #loading X file from X.pickle\n",
    "y = pickle.load(open(\"y.pickle\",\"rb\"))    #loading y file from y.pickle\n",
    "\n",
    "X= X/255.0                    #deviding X by 255 to neutralize it\n",
    "\n",
    "X = np.array(X)               #converting X into numpy array\n",
    "y = np.array(y)               #converting y into numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, keras Sequential model has been used to make Convolutional neural network. There are two convolutional layers with the 64 neurons each and with (3,3) window size. The activation function in both convolutional layers is \"relu\". Then MaxPooling2D is used in both of the convolutional layers with pool size of (2,2). Then there is one Dense layer of 64 neurons and before that flatten is used because the Dense layer only accepts 1D data. The output layer is also a Dense layer with \"softmax\" activation and 6 neurons. Then in \"model.compile\", \"sparse_categorical_crossentropy\" is used to reduce loss, \"adam\" is used as an optimizer and 'accuracy' is used for metrics. Then validation split is 0.1, epochs is 5 and batch size is 32 in the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12630 samples, validate on 1404 samples\n",
      "Epoch 1/5\n",
      "12630/12630 [==============================] - 239s 19ms/sample - loss: 1.2247 - accuracy: 0.5290 - val_loss: 0.9551 - val_accuracy: 0.6311\n",
      "Epoch 2/5\n",
      "12630/12630 [==============================] - 238s 19ms/sample - loss: 0.7680 - accuracy: 0.7162 - val_loss: 0.6918 - val_accuracy: 0.7443\n",
      "Epoch 3/5\n",
      "12630/12630 [==============================] - 241s 19ms/sample - loss: 0.5825 - accuracy: 0.7918 - val_loss: 0.6915 - val_accuracy: 0.7443\n",
      "Epoch 4/5\n",
      "12630/12630 [==============================] - 243s 19ms/sample - loss: 0.4095 - accuracy: 0.8605 - val_loss: 0.7853 - val_accuracy: 0.7279\n",
      "Epoch 5/5\n",
      "12630/12630 [==============================] - 246s 19ms/sample - loss: 0.2402 - accuracy: 0.9188 - val_loss: 0.8173 - val_accuracy: 0.7472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x76e6903d0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()                                #making keras sequential model\n",
    "model.add(Conv2D(64,(3,3), input_shape =X.shape[1:])) #Adding convolutional neural network with window size of (3,3) and 64 neurons\n",
    "model.add(Activation(\"relu\"))                         #Adding activation function as \"relu\"    \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))              #Adding pooling layer with pool size of (2,2) and pooling function of MaxPoolin2D\n",
    "\n",
    "model.add(Conv2D(64,(3,3), input_shape =X.shape[1:])) #Adding convolutional neural network with window size of (3,3) and 64 neurons\n",
    "model.add(Activation(\"relu\"))                         #Adding activation function as \"relu\"\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))              #Adding pooling layer with pool size of (2,2) and pooling function of MaxPoolin2D\n",
    "\n",
    "model.add(Flatten())                         #Adding layer to flatten the data\n",
    "model.add(Dense(64))                         #Adding Dense layer with 64 neurons\n",
    "\n",
    "model.add(Dense(6))                          #Adding output layer with 6 neurons \n",
    "model.add(Activation(\"softmax\"))             #Adding activation function of \"softmax\" to the output layer\n",
    "\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", #minimizing loss with \"sparse_categorical_crossentropy\"\n",
    "             optimizer = \"adam\",                        #optimizing with \"adam\"\n",
    "             metrics =['accuracy'])                     #Adding metrics as 'accuracy'\n",
    "\n",
    "model.fit(X, y, batch_size=32, validation_split =0.1, epochs =5) #selecting batch size of 32, validation split of 0.1 and 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, After getting the optimum result by changing the number of layers and number of neurons, the keras model with its optimum result is saved in \"HDF5\" file with \"model.save\" and \"h5py\". Also loading the same model with \"load_model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py                         #importing h5py                   \n",
    "from keras.models import load_model #importing load_model\n",
    "model.save('my_model3.h5')         #saving model in \"HDF5\" file as \"my_model3.h5\"\n",
    "del model                          #deleting existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model \n",
    "classifierLoad = tf.keras.models.load_model('/Users/vishvmac/Machine Learning/Machine learning projects/Intel image classification dataset/my_model3.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loading the test dataset, with \"pickle.load\" that has been saved before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pickle.load(open(\"X_test.pickle\",\"rb\"))      #loading X_test file from X_test.pickle\n",
    "y_test = pickle.load(open(\"y_test.pickle\",\"rb\"))      #loading y_test file from y_test.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loading the libraries of \"sklearn.metrics\" to get the \"confusion matrix\" and \"accuracy score\". And getting the rounded predictions as the direct predictions from the model will be probabilities of different classes. and then printing confusion metrics as cm and accuracy score as acc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix        #importing \"confusion_matrix\" from \"sklearn.metrics\" \n",
    "from sklearn.metrics import accuracy_score          #importing \"accuracy_score\" from \"sklearn.metrics\" \n",
    "\n",
    "X_test = tf.cast(X_test, tf.float32)                #converting X_test into float32.\n",
    "rounded_predictions = classifierLoad.predict_classes(X_test,batch_size=32 ,verbose=0) #getting rounded predictions from model\n",
    "    \n",
    "cm = confusion_matrix(y_test, rounded_predictions)  #getting confusion matrix \n",
    "acc_score = accuracy_score(y_test, rounded_predictions) #getting accuracy score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[224   9  85  25  27  67]\n",
      " [ 13 376  51   9   6  19]\n",
      " [  4   0 481  55  13   0]\n",
      " [  8   4 257 243   7   6]\n",
      " [ 13   0 248  89 150  10]\n",
      " [ 69  29  93  16   8 286]] 0.5866666666666667\n"
     ]
    }
   ],
   "source": [
    "print(cm,acc_score)  #printing confusion matrix and accuracy score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
